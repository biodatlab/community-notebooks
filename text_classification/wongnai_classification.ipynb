{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pandas-profiling\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install sentencepiece\n",
    "!pip install -U scikit-learn\n",
    "!pip install accelerate\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory for the downloaded dataset.\n",
    "dataset_name = \"wongnai-dataset\"\n",
    "os.makedirs(dataset_name, exist_ok=True) \n",
    "\n",
    "# Download the dataset from google drive.\n",
    "!wget https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip\n",
    "\n",
    "# Unzip the dataset.\n",
    "!unzip review_dataset.zip -d wongnai-dataset # for linux\n",
    "# !tar -xzvf review_dataset.zip -C wongnai-dataset # for windows\n",
    "\n",
    "# Remove the zip file.\n",
    "!rm review_dataset.zip\n",
    "# Remove the unrelated __MACOSX folder.\n",
    "!rm -r wongnai-dataset/__MACOSX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"wongnai-dataset/w_review_train.csv\", \n",
    "    sep=\";\",\n",
    "    names=[\"review\", \"rating\"],\n",
    "    header=None\n",
    ")\n",
    "# Remove duplicate rows.\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Remove newline (\\n) characters from reviews.\n",
    "df[\"review\"] = df[\"review\"].str.replace('\\n','') # remove \\n\n",
    "# Create label column from rating column.\n",
    "df[\"label\"] = df[\"rating\"].map({1: 0, 2: 1, 3: 2, 4: 3, 5: 4})\n",
    "# Drop rating column because we only want review and label columns.\n",
    "df.drop(\"rating\", axis=1, inplace=True)\n",
    "# Sample dataset for speed up training. Comment this line if you want to use the whole dataset.\n",
    "df = df.sample(3000)\n",
    "# Split the dataset into train and test sets.\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42) # Random state is just for reproducibility.\n",
    "# Save the train and test sets to csv files.\n",
    "train_df.to_csv(\"./train.csv\", index=False) \n",
    "val_df.to_csv(\"./test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the csv files.\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\" : \"test.csv\" })\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Load accuracy metric.\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Create a dictionary of rating labels and their corresponding ids.\n",
    "id2label = {0: \"Very poor (1) \", 1: \"Poor (2)\", 2: \"Average (3)\", 3: \"Good (4)\", 4: \"Excellent (5)\"}\n",
    "label2id = {\"Very poor (1)\": 0, \"Poor (2)\": 1, \"Average (3)\": 2, \"Good (4)\": 3, \"Excellent (5)\": 4} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "\n",
    "# Load pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    num_labels=5, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Freeze the weights of the encoder and only train the classification head and the last layer of the encoder.\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.roberta.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    use_fast=False, \n",
    "    model_max_length = 256 # The maximum length (in number of tokens) for the inputs to the transformer model\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"\n",
    "    Function to convert the review texts to tokens.\n",
    "    For example: \"I like this food\" -> [101, 146, 108, 114, 117, 110, 170, 102]\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# Applying the tokenize function to the our dataset.\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Data collator is used to create batches of input data from the dataset.\n",
    "# DataCollatorWithPadding will dynamically pad the tokens to the maximum length in that batch.\n",
    "# More info on data collators: https://www.youtube.com/watch?v=-RPeakdlHYo\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our first sample in the training set.\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Create a TrainingArguments object to configure how to train the model.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Wongnai_classification\", # Where to store the final model.\n",
    "    learning_rate=5e-5,  # Set learning rate\n",
    "    per_device_train_batch_size=64,  # Batch size for training.\n",
    "    per_device_eval_batch_size=64, # Batch size for evaluation.\n",
    "    num_train_epochs=10,  # Number of training epochs.\n",
    "    weight_decay=0.01, \n",
    "    evaluation_strategy=\"epoch\", # How often to evaluate the model.\n",
    "    save_strategy=\"epoch\", # How often to save the model.\n",
    "    load_best_model_at_end=True, # Whether to load the best model at the end of training.\n",
    "    push_to_hub=False, # Whether to upload the final model to the Huggingface Hub.\n",
    "    report_to=\"none\", # Whether to report logging to any services.\n",
    "    fp16=True # Use mixed precision to speed up training.\n",
    ")\n",
    "\n",
    "# Create a trainer to train the model.\n",
    "trainer = Trainer( \n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = (\n",
    "\"ปกติมาทานบ่อยอยู่แล้ว วันนี้ลองมาทานก๋วยเตี๋ยวเจดู \"\n",
    "\"รสชาติสู้แบบปกติไม่ได้เพราะปกติอร่อยมากๆ แต่พอเป็นแนวเจ \" \n",
    "\"รสชาติของเครื่องจึงตกลง แต่น้ำซุปยังอร่อยเหมือนเดิม ส่วนไอศครีมกะทิราดซุปข้าวโพด \"\n",
    "\"อร่อยดี ทานไม่บ่อย แต่คราวนี้ลองดู ช่วยให้มื้อนี้ดูดีขึ้นเยอะเลย\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pipeline to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Point to the directory where the model weight is stored.\n",
    "model_dir = \"Wongnai_classification/checkpoint-8000/\"\n",
    "# Create a pipeline to classify the sentiment of the input text.\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_dir)\n",
    "# Let's try it out!\n",
    "output = classifier(example_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Point to the directory where the model weight is stored.\n",
    "model_dir = \"Wongnai_classification/checkpoint-8000/\"\n",
    "\n",
    "# Create model from our fine-tuned checkpoint.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_dir, \n",
    "    local_files_only=True # Look for the model in the local directory, not on the Huggingface Hub.\n",
    ")\n",
    "# Load tokenizer from our pretrained model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Tokenize the example text to input_ids.\n",
    "input_ids = tokenizer(example_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation because we are not training\n",
    "    logits = model(**input_ids).logits\n",
    "\n",
    "# Get the most probable class for the input text.\n",
    "predicted_class_id = logits.argmax().item()\n",
    "# Get the rating label from the predicted rating\n",
    "predicted_rating_label = model.config.id2label[predicted_class_id]\n",
    "\n",
    "\n",
    "print(f\"Input review: {example_text}\\nPredicted rating label: \\\"{predicted_rating_label}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap with gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nice interactive demo, we use gradio to wrap our model and make it easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio.components import Textbox, Label\n",
    "from gradio import Interface\n",
    "\n",
    "# Define a function that will process the input text from gradio widget.\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify the sentiment of the input text and return the predicted rating.\"\"\"\n",
    "    output = classifier(text)\n",
    "    prediction = output[0]\n",
    "\n",
    "    predicted_rating = prediction[\"label\"]\n",
    "    confidence_score = round(prediction[\"score\"], 4)\n",
    "    return predicted_rating, confidence_score\n",
    "\n",
    "\n",
    "# Create a gradio interface.\n",
    "rating_inferface = Interface(\n",
    "    fn=classify_text,\n",
    "    inputs=Textbox(label=\"Review\"),\n",
    "    outputs=[\n",
    "        Label(label=\"Predicted rating\"),\n",
    "        Label(label=\"Confidence score\"),\n",
    "    ],\n",
    "\n",
    "    title=\"Wongnai review rating prediction\",\n",
    "    description=\"Classify the sentiment of the review into the predicted rating with confidence score.\",\n",
    ")\n",
    "rating_inferface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objdetyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
