{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pandas-profiling\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install sentencepiece\n",
    "!pip install -U scikit-learn\n",
    "!pip install accelerate\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-26 22:15:07--  https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/wongnai/wongnai-corpus/master/review/review_dataset.zip [following]\n",
      "--2023-08-26 22:15:07--  https://raw.githubusercontent.com/wongnai/wongnai-corpus/master/review/review_dataset.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14338706 (14M) [application/zip]\n",
      "Saving to: ‘review_dataset.zip’\n",
      "\n",
      "review_dataset.zip  100%[===================>]  13.67M  16.8MB/s    in 0.8s    \n",
      "\n",
      "2023-08-26 22:15:08 (16.8 MB/s) - ‘review_dataset.zip’ saved [14338706/14338706]\n",
      "\n",
      "Archive:  review_dataset.zip\n",
      " extracting: wongnai-dataset/sample_submission.csv  \n",
      "  inflating: wongnai-dataset/test_file.csv  \n",
      "  inflating: wongnai-dataset/w_review_train.csv  \n",
      "   creating: wongnai-dataset/__MACOSX/\n",
      "  inflating: wongnai-dataset/__MACOSX/._sample_submission.csv  \n",
      "  inflating: wongnai-dataset/__MACOSX/._test_file.csv  \n",
      "  inflating: wongnai-dataset/__MACOSX/._w_review_train.csv  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory for the downloaded dataset.\n",
    "dataset_name = \"wongnai-dataset\"\n",
    "os.makedirs(dataset_name, exist_ok=True) \n",
    "\n",
    "# Download the dataset from google drive.\n",
    "!wget https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip\n",
    "\n",
    "# Unzip the dataset.\n",
    "!unzip review_dataset.zip -d wongnai-dataset # for linux\n",
    "# !tar -xzvf review_dataset.zip -C wongnai-dataset # for windows\n",
    "\n",
    "# Remove the zip file.\n",
    "!rm review_dataset.zip\n",
    "# Remove the unrelated __MACOSX folder.\n",
    "!rm -r wongnai-dataset/__MACOSX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"wongnai-dataset/w_review_train.csv\", \n",
    "    sep=\";\",\n",
    "    names=[\"review\", \"rating\"],\n",
    "    header=None\n",
    ")\n",
    "# Remove duplicate rows.\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Remove newline (\\n) characters from reviews.\n",
    "df[\"review\"] = df[\"review\"].str.replace('\\n','') # remove \\n\n",
    "# Create label column from rating column.\n",
    "df[\"label\"] = df[\"rating\"].map({1: 0, 2: 1, 3: 2, 4: 3, 5: 1})\n",
    "# Drop rating column because we only want review and label columns.\n",
    "df.drop(\"rating\", axis=1, inplace=True)\n",
    "# Sample 1000 rows for the sake of example.\n",
    "df = df.sample(1000)\n",
    "# Split the dataset into train and test sets.\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42) # Random state is just for reproducibility.\n",
    "# Save the train and test sets to csv files.\n",
    "train_df.to_csv(\"./train.csv\", index=False) \n",
    "val_df.to_csv(\"./test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nim/miniforge3/envs/wongnai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 9436.00it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 813.09it/s]\n",
      "Generating train split: 800 examples [00:00, 19861.86 examples/s]\n",
      "Generating test split: 200 examples [00:00, 28385.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the csv files.\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\" : \"test.csv\" })\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Load accuracy metric.\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Create a dictionary of rating labels and their corresponding ids.\n",
    "id2label = {0: \"Very poor (1) \", 1: \"Poor (2)\", 2: \"Average (3)\", 3: \"Good (4)\", 4: \"Exellent (5)\"}\n",
    "label2id = {\"Very poor (1)\": 0, \"Poor (2)\": 1, \"Average (3)\": 2, \"Good (4)\": 3, \"Exellent (5)\": 4} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 1604.44 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1684.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "\n",
    "# Load pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    num_labels=5, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    use_fast=False, \n",
    "    model_max_length = 100 # The maximum length (in number of tokens) for the inputs to the transformer model\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"\n",
    "    Function to convert the review texts to tokens.\n",
    "    For example: \"I like this food\" -> [101, 146, 108, 114, 117, 110, 170, 102]\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# Applying the tokenize function to the our dataset.\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Data collator is used to create batches of input data from the dataset.\n",
    "# DataCollatorWithPadding will dynamically pad the tokens to the maximum length in that batch.\n",
    "# More info on data collators: https://www.youtube.com/watch?v=-RPeakdlHYo\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'วันดี วันที่สมุนครบทีม (มีบางคนแซวว่า เด็กในฮาเร็ม 555)เกณฑ์พลมาที่แยกประตูน้ำ หัวมุมทางไป Platinamแต่ละหน่อ เอาใจลำบาก / มาแบบนี้ก็ดี จะได้เลือกกินกันเองไลน์อาหารมากพอควร ...ถูกใจที่หมูกรอบ หมูแดง เฉือนเสิร์ฟ สดๆ นุ่มมมมม ซะขอบอกว่า ---> ไลน์ของหวาน เพียบ', 'label': 2, 'input_ids': [5, 10, 125, 81, 2573, 22238, 1497, 230, 1241, 23, 1042, 7294, 27, 6345, 20, 901, 8738, 76, 14432, 29, 3177, 172, 26, 12, 818, 20031, 10, 303, 1399, 75, 28, 10, 3, 1663, 2657, 11804, 953, 13962, 10, 6708, 2036, 10, 101, 10, 26, 227, 2575, 10, 492, 309, 241, 3207, 2035, 319, 82, 20489, 4748, 4608, 12, 18502, 10, 20838, 10, 16444, 4858, 10, 10258, 10, 2145, 2139, 2139, 10, 964, 20626, 10, 8156, 1992, 10, 2035, 10385, 10, 5471, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at our first sample in the training set.\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Create a TrainingArguments object to configure how to train the model.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Wongnai_classification\", # Where to store the final model.\n",
    "    learning_rate=0.00001,  # Set learning rate\n",
    "    per_device_train_batch_size=1,  # Batch size for training.\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation.\n",
    "    num_train_epochs=10,  # Number of training epochs.\n",
    "    weight_decay=0.01, \n",
    "    evaluation_strategy=\"epoch\", # How often to evaluate the model.\n",
    "    save_strategy=\"epoch\", # How often to save the model.\n",
    "    load_best_model_at_end=True, # Whether to load the best model at the end of training.\n",
    "    push_to_hub=False, # Whether to upload the final model to the Huggingface Hub.\n",
    "    report_to=\"none\" # Whether to report logging to any services.\n",
    ")\n",
    "\n",
    "# Create a trainer to train the model.\n",
    "trainer = Trainer( \n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = (\n",
    "\"ปกติมาทานบ่อยอยู่แล้ว วันนี้ลองมาทานก๋วยเตี๋ยวเจดู \"\n",
    "\"รสชาติสู้แบบปกติไม่ได้เพราะปกติอร่อยมากๆ แต่พอเป็นแนวเจ \" \n",
    "\"รสชาติของเครื่องจึงตกลง แต่น้ำซุปยังอร่อยเหมือนเดิม ส่วนไอศครีมกะทิราดซุปข้าวโพด \"\n",
    "\"อร่อยดี ทานไม่บ่อย แต่คราวนี้ลองดู ช่วยให้มื้อนี้ดูดีขึ้นเยอะเลย\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pipeline to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Point to the directory where the model weight is stored.\n",
    "model_dir = \"Wongnai_classification/checkpoint-8000/\"\n",
    "# Create a pipeline to classify the sentiment of the input text.\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_dir)\n",
    "# Let's try it out!\n",
    "output = classifier(example_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Point to the directory where the model weight is stored.\n",
    "model_dir = \"Wongnai_classification/checkpoint-8000/\"\n",
    "\n",
    "# Create model from our fine-tuned checkpoint.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_dir, \n",
    "    local_files_only=True # Look for the model in the local directory, not on the Huggingface Hub.\n",
    ")\n",
    "# Load tokenizer from our pretrained model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Tokenize the example text to input_ids.\n",
    "input_ids = tokenizer(example_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation because we are not training\n",
    "    logits = model(**input_ids).logits\n",
    "\n",
    "# Get the most probable class for the input text.\n",
    "predicted_class_id = logits.argmax().item()\n",
    "# Get the rating label from the predicted rating\n",
    "predicted_rating_label = model.config.id2label[predicted_class_id]\n",
    "\n",
    "\n",
    "print(f\"Input review: {example_text}\\nPredicted rating label: \\\"{predicted_rating_label}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap with gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nice interactive demo, we use gradio to wrap our model and make it easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio.components import Textbox, Label\n",
    "from gradio import Interface\n",
    "\n",
    "# Define a function that will process the input text from gradio widget.\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify the sentiment of the input text and return the predicted rating.\"\"\"\n",
    "    output = classifier(text)\n",
    "    prediction = output[0]\n",
    "\n",
    "    predicted_rating = prediction[\"label\"]\n",
    "    confidence_score = round(prediction[\"score\"], 4)\n",
    "    return predicted_rating, confidence_score\n",
    "\n",
    "\n",
    "# Create a gradio interface.\n",
    "rating_inferface = Interface(\n",
    "    fn=classify_text,\n",
    "    inputs=Textbox(label=\"Review\"),\n",
    "    outputs=[\n",
    "        Label(label=\"Predicted rating\"),\n",
    "        Label(label=\"Confidence score\"),\n",
    "    ],\n",
    "\n",
    "    title=\"Wongnai review rating prediction\",\n",
    "    description=\"Classify the sentiment of the review into the predicted rating with confidence score.\",\n",
    ")\n",
    "rating_inferface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objdetyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
