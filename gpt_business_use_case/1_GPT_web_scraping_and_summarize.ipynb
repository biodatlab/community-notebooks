{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Use Case 1: Web-scraping and content summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and declare API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\" # select model based on application\n",
    "openai.api_key = API_key = 'sk-...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions for web-scraping and GPT execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping(url: str) -> list[str]:\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    list_contents = [contents.get_text() for contents in soup.find_all(string=True)] # access all strings existed within a website\n",
    "    return list_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt: str) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    result = response['choices'][0]['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summarize_template(strings: str) -> str:\n",
    "    summarize_template = (\\\n",
    "        \"\"\"You have no previous memories.\\n\"\"\"\n",
    "        \"\"\"You are professional in summarizing contents with rational logic.\\n\"\"\"\n",
    "        \"\"\"Your task is to summarize provided web-content into a well-structured list of information.\\n\"\"\"\n",
    "        f\"\"\"web-content: {strings}\\n\"\"\"\n",
    "        \"\"\"Please return the response in JSON schema with the following keys:\\n\"\"\"\n",
    "        \"\"\"'title', 'description', 'price', 'mileage', 'brand', 'model', 'year', 'color', 'fuel_type', 'transmission', 'location', 'contact'\\n\"\"\"\n",
    "    )\n",
    "    return summarize_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web-scraping and run GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of web urls\n",
    "list_url = [\\\n",
    "    'https://rod.kaidee.com/product-368200692',\n",
    "    'https://rod.kaidee.com/product-368040520',\n",
    "    'https://rod.kaidee.com/product-367246566']\n",
    "    \n",
    "# Declare path for result export\n",
    "export_path = './1_GPT_web_scraping_and_summarize_report'\n",
    "csv_path = os.path.join(export_path, 'web_content_summary.csv')\n",
    "\n",
    "# Clear existed csv file\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "\n",
    "# Extract contents from url and run GPT\n",
    "for url in list_url:\n",
    "    contents = web_scraping(url)\n",
    "    summarize_template = create_summarize_template(contents)\n",
    "    result = run_prompt(summarize_template)\n",
    "\n",
    "# Format response into DataFrame\n",
    "    summarized_content = json.loads(result)\n",
    "    summarized_df = pd.DataFrame.from_dict(summarized_content.items())\n",
    "\n",
    "    empty_row = pd.DataFrame(None, index=[0]) # Append empty row\n",
    "    original_url = pd.DataFrame({'url': url}, index=[0]) # Append url of the response\n",
    "\n",
    "    temp_summarized_df = pd.concat([original_url,summarized_df.loc[:]]).reset_index(drop=True)\n",
    "    final_summarized_df = pd.concat([empty_row,temp_summarized_df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "# Export to csv file\n",
    "    final_summarized_df.to_csv(csv_path, mode='a', encoding='utf-8-sig', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate web-scraping code using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generating_prompt = (\n",
    "    \"\"\"Write python code for web-scraping to find all strings using BeautifulSoup4 library\"\"\"\n",
    "    \"\"\"Use urllib to access website with headers\"\"\"\n",
    "    \"\"\"Declare url to scrap as 'https://rod.kaidee.com/product-368200692'\"\"\"\n",
    ")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'user', \"content\": code_generating_prompt}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "generated_command = response['choices'][0]['message']['content']\n",
    "print(generated_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScrapPrompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
